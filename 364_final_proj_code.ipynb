{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b412b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e618ec",
   "metadata": {},
   "source": [
    "# Set the seed for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ee90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b359ddc9",
   "metadata": {},
   "source": [
    "# Defining functions for processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d50e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(text):\n",
    "  \"\"\"\n",
    "  Simple tokenizer that lowercases and splits on whitespace.\n",
    "  So if we input \"Hello World!\" it will return [\"hello\", \"world!\"]\n",
    "  \"\"\"\n",
    "  return text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa997c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts, min_freq=1):\n",
    "  \"\"\"\n",
    "  Build vocabulary on a list/series of text\n",
    "  This allows us to convert text into a series of numbers\n",
    "  This is an easier data that can be read by the model for patterns\n",
    "  So if we input [\"Hello world\", \"AI is amazing\", \"AI for the future\"]\n",
    "  It will return {\"<pad>\": 0, \"<unk>\": 1, \"hello\": 2, \"world\": 3, \"ai\": 4, \"is\": 5, \"amazing\": 6, \"for\": 7, \"the\": 8, \"future\": 9}\n",
    "  \"\"\"\n",
    "  vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "  word_counts = {}\n",
    "  for text in texts:\n",
    "      tokens = simple_tokenizer(text)\n",
    "      for token in tokens:\n",
    "          word_counts[token] = word_counts.get(token, 0) + 1\n",
    "  for token, count in word_counts.items():\n",
    "      if count >= min_freq and token not in vocab:\n",
    "          vocab[token] = len(vocab)\n",
    "  return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a67fe9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text, vocab):\n",
    "  \"\"\"\n",
    "  Convert text to a sequence of numbers\n",
    "  Using the vocab dictionary created by the build_vocab function\n",
    "  We can convert the text into a series of numbers by mapping each word to its number\n",
    "  Also if we encounter a word that is not in the vocab, we map it to the <unk> token\n",
    "  So if we input \"Hello Bob World\" and the vocab is {\"<pad>\": 0, \"<unk>\": 1, \"hello\": 2, \"world\": 3}\n",
    "  It will return [2, 1, 3]\n",
    "  \"\"\"\n",
    "  tokens = simple_tokenizer(text)\n",
    "  sequence = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "  return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8eefe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequence, max_length):\n",
    "  \"\"\"\n",
    "  This will ensure that all sequences are the same length\n",
    "  By padding the shorter sequences with the <pad> token\n",
    "  since models require consistent sizes of input\n",
    "  So if the input sequences are [1, 2, 3] and the max length is 5\n",
    "  It will return [1, 2, 3, 0, 0]\n",
    "\n",
    "  \"\"\"\n",
    "  if len(sequence) < max_length:\n",
    "      sequence += [0] * (max_length - len(sequence))\n",
    "  else:\n",
    "      sequence = sequence[:max_length]\n",
    "  return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ed06d",
   "metadata": {},
   "source": [
    "# Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "  \"\"\"\n",
    "  Custom Pytorch dataset class for the tweet data\n",
    "  Converts all the text data into something that can be read by the model\n",
    "  This also converts the dataframe into a Pytorch tensor\n",
    "  \"\"\"\n",
    "  def __init__(self, texts, labels, vocab, max_len):\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.vocab = vocab\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # If texts/labels are pandas Series, use .iloc for proper indexing.\n",
    "    text_item = self.texts.iloc[idx] if isinstance(self.texts, pd.Series) else self.texts[idx]\n",
    "    label_item = self.labels.iloc[idx] if isinstance(self.labels, pd.Series) else self.labels[idx]\n",
    "    sequence = text_to_sequence(text_item, self.vocab)\n",
    "    padded_sequence = pad_sequence(sequence, self.max_len)\n",
    "    return torch.tensor(padded_sequence, dtype=torch.long), torch.tensor(label_item, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1fcfdc",
   "metadata": {},
   "source": [
    "# Design the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924538b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_Classifier(nn.Module):\n",
    "  \"\"\"\n",
    "  This will be an LSTM-based binary classifier\n",
    "\n",
    "  Architecture:\n",
    "  - Embedding layer: This will convert the input sequence of numbers into a sequence of vectors\n",
    "  - LSTM layer: This will learn the patterns in the input sequence\n",
    "  - Linear layer: This will map the LSTM output to a single output\n",
    "  - Dropout layer: This will help prevent overfitting by randomly dropping out some of the neurons during training\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes=2):\n",
    "    super(Binary_Classifier, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "    self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    self.dropout = nn.Dropout(p=0.6)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    x, _ = self.lstm(x)\n",
    "    x = self.dropout(x[:, -1, :])\n",
    "    logits = self.fc(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d2f8a",
   "metadata": {},
   "source": [
    "# Design the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2587826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer):\n",
    "  \"\"\"\n",
    "  Train the model and evaluate on the validation set after each epoch.\n",
    "    \n",
    "  Args:\n",
    "  - model (nn.Module): The LSTM classifier.\n",
    "  - train_loader (DataLoader): DataLoader for training data.\n",
    "  - val_loader (DataLoader): DataLoader for validation data.\n",
    "  - epochs (int): Number of training epochs.\n",
    "  - criterion: Loss function.\n",
    "  - optimizer: Optimization algorithm.\n",
    "\n",
    "  \"\"\"\n",
    "  val_losses = []\n",
    "  train_losses = []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for input_ids, labels in train_loader:\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(input_ids)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Training Loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "      for input_ids, labels in val_loader:\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Validation Loss: {avg_val_loss:.4f}')\n",
    "  \n",
    "  # Return the results of training\n",
    "  return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fc1bf",
   "metadata": {},
   "source": [
    "# Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv(\"valid.tsv\", sep='\\t', names=[\"Id\", \"Text\", \"Label\"])\n",
    "train_df = pd.read_csv(\"train.tsv\", sep='\\t', names=[\"Id\", \"Text\", \"Label\"])\n",
    "test_df = pd.read_csv(\"test.tsv\", sep='\\t', names=[\"Id\", \"Text\", \"Label\"])\n",
    "noisy_df = pd.read_csv(\"unlabeled_test_with_noise.tsv\", sep='\\t', names=[\"Id\", \"Text\", \"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e05ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some cleaning of train data so model trains properly\n",
    "train_df.drop(index=train_df.index[0], axis=0, inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67355ff8",
   "metadata": {},
   "source": [
    "# Initial Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb509ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the string label to binary values of 0 and 1\n",
    "label_mapping = {\"UNINFORMATIVE\": 0, \"INFORMATIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d33250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the label mapping to all the datasets\n",
    "valid_df[\"Label\"] = valid_df[\"Label\"].map(label_mapping)\n",
    "noisy_df[\"Label\"] = noisy_df[\"Label\"].map(label_mapping)\n",
    "train_df[\"Label\"] = train_df[\"Label\"].map(label_mapping)\n",
    "test_df[\"Label\"] = test_df[\"Label\"].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c299d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary based on training texts\n",
    "vocab = build_vocab(train_df[\"Text\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b54dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum token sequence length (adjust as needed)\n",
    "max_len = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd31e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate texts and labels from dataframes\n",
    "train_texts, train_labels = train_df[\"Text\"], train_df[\"Label\"]\n",
    "valid_texts, valid_labels = valid_df[\"Text\"], valid_df[\"Label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96956e44",
   "metadata": {},
   "source": [
    "# Create Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd88b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Datasets and DataLoaders\n",
    "train_dataset = TweetDataset(train_texts, train_labels, vocab, max_len)\n",
    "valid_dataset = TweetDataset(valid_texts, valid_labels, vocab, max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd96e1",
   "metadata": {},
   "source": [
    "# Initialize and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d570f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 75\n",
    "hidden_dim = 96\n",
    "model = Binary_Classifier(vocab_size, embed_dim, hidden_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4720b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training for a specified number of epochs\n",
    "epochs = 10\n",
    "train_losses, val_losses = train_model(model, train_loader, valid_loader, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606a3fa",
   "metadata": {},
   "source": [
    "# Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2168aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d310fb",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66dcdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the test dataset using our dataset class\n",
    "# Create a copy of test dataframe for consistentcy\n",
    "result_test_df = test_df.copy()\n",
    "\n",
    "max_len = 45  # must match what was used during training\n",
    "test_texts, test_labels = result_test_df[\"Text\"], result_test_df[\"Label\"]\n",
    "\n",
    "# Create the dataset and loader\n",
    "test_dataset = TweetDataset(test_texts, test_labels, vocab, max_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Container for predictions\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  for input_ids, _ in test_loader:\n",
    "    outputs = model(input_ids)  # outputs of shape [batch_size, num_classes]\n",
    "    # Convert logits to predicted labels by taking argmax along the logits dimension\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# Convert predictions to a numpy array (just in case)\n",
    "all_preds = np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d432bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and output the dataframe\n",
    "\n",
    "# Add predictions to your DataFrame. You can also convert numbers back to string labels if needed.\n",
    "result_test_df[\"predicted Label\"] = all_preds\n",
    "\n",
    "# Map the predictions back to string labels:\n",
    "reverse_label_mapping = {0: \"UNINFORMATIVE\", 1: \"INFORMATIVE\"}\n",
    "result_test_df[\"predicted Label\"] = result_test_df[\"predicted Label\"].map(reverse_label_mapping)\n",
    "result_test_df[\"Label\"] = result_test_df[\"Label\"].map(reverse_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5284532",
   "metadata": {},
   "source": [
    "# Accuracy check on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "different_rows = len(result_test_df[result_test_df[\"Label\"] != result_test_df[\"predicted Label\"]])\n",
    "total_rows = len(result_test_df)\n",
    "print(f\"Number of different rows: {different_rows}\")\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "print(f\"Model Accuracy: {100 * (1 - different_rows / total_rows)} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
